
# Data Check - Thematic area: Proportionality and Do No Harm

## Definitions

Proportionality can be defined as ‘the fact or quality of being in proper balance or relation as to size or quantity, degree, severity, etc.’. As a foundational principle of international law, proportionality supports the need for actions to be both fair and just. In the context of AI, proportionality demands the use of AI systems to not ‘exceed what is necessary to achieve legitimate aims or objectives and should also be appropriate to the context’.
Similarly, the principle of 'do no harm' requires actions not to cause injury or injustice to people. When read together, the principle of proportionality and ‘do no harm’ require AI systems not to be used in ways that cause or exacerbate harm, whether at an individual or communal level, and includes injuries and injustices that impact on the social, cultural, economic, natural or political environments. Further, it should also be noted that the principle of ‘do no harm’ (non-maleficence) is distinct from the principle of ‘do only good’ (beneficence), where the latter concerns the creation of AI technology in ways that benefit humanity and the planet, while ‘do no harm’ concerns safeguarding against the overuse or misuse of AI technologies.


## Identifications

This thematic area measures steps countries have taken to integrate the principles of proportionality and do no harm into AI systems, and to implement safeguards in the design and operation of AI systems. In particular, evidence must account for (1) frameworks concerning provisions or mechanisms for assessing proportionality and commitments upholding the principle of ‘do no harm’ throughout all phases of the AI lifecycle, (2) government actions regulating or promoting the principles of proportionality and do no harm in AI systems through legal or other means, and (3) non-state actors involved in promoting the principle of proportionality and do no harm in the design, development and use of AI systems.
Frameworks may take the form of laws, regulations, policies and/or guidelines. Government actions may include draft laws or policies, the creation of expert working groups to provide policy recommendations on how to integrate the principles of proportionality and do no harm into the regulatory frameworks governing AI, as well as the establishment of oversight bodies to monitor compliance by AI users. Non-governmental actors or non-state actors (NSAs) may include non-governmental organisations (NGOs), but also multinational corporations, private military organisations, media outlets, organised ethnic groups, academic institutions, lobby groups, labour unions or social movements working to promote the principle of proportionality and do no harm in the implementation of AI technologies.

## Some Examples:

### Frameworks:

Article 5 of the European Union’s General Data Protection Regulation (GDPR) provides a mechanism for upholding the principle of proportionality by establishing a ‘data minimisation’ principle which requires the handling of personal data to be ‘adequate, relevant and limited to what is necessary to the purposes for which it is processed’. The data minimisation principle is further expanded in Recital 78, which positions the ‘minimisation of personal data’ as an organisational measure for data protection by design and by default. In this regard, the data minimisation principle aligns with or upholds the principle of proportionality in that it does not ban the processing of additional personal data if its inclusion offers a benefit to the processing purpose that outweighs the additional risks for the data subjects. Together, evaluating data controller activity on the basis of such principles can be said to constitute a type of ‘proportionality testing’ with respect to secondary data protection law.

### Government Actions

The UK government has invested £250 million in the NHS AI Lab, an initiative which seeks to address the challenges and barriers to developing and deploying AI systems in healthcare settings by facilitating engagements between different stakeholders. In an effort to foster innovation while ensuring proper guidance and support to protect patients, the initiative has offered “learning opportunities to increase confidence” as well as “researching ways to minimise the potential risks.” Accordingly, the NHS AI Lab has published a report with Health Education England “identifying educational and training needs” to develop workplace confidence in AI and a report on the use of an algorithmic impact assessment for data access in healthcare, in collaboration with the Ada Lovelace Institute; created an online resource library of AI information for health and care; and launched the NHS AI Lab Ethics Initiative to direct policy on “ethical assurance of AI.”

### Non-state Actors

The International Committee of the Red Cross (ICRC) has published a position and background paper on the use of automatic weapons systems which urges countries to ‘establish internationally agreed limits on autonomous weapon systems to ensure civilian protection, compliance with international humanitarian law, and ethical acceptability’. In particular, the ICRC recommends that countries work together to adopt a set of legally binding rules and establish various measures with relevant stakeholders at international and national level to support government initiatives that realise these measures. Meanwhile, the campaign ‘Stop Killer Robots,’ a global coalition launched in 2013 currently consisting of over 180 member civil society organisations seeking to resist ‘digital dehumanisation,' is calling for “new international law on autonomy in weapons systems” that would “ensure human control in the use of force,” thus preventing the development and deployment of fully autonomous systems.
