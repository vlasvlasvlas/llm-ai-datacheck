
# Data Check - Thematic area: Foundational Models

## Definitions

Foundational models, commonly referred to as ‘Generative AI’, are a type of artificial intelligence that describes algorithms whose primary function is to generate content, including text, audio, code, images, simulations and videos. Unlike traditional AI systems, which rely on pre-defined rules and patterns, foundational models use deep learning techniques and large datasets to mimic human behavior to produce original content without human intervention based on what the models have learned. Kindly note that for purposes of this thematic area, the term ‘foundational models’ and ‘generative AI’ are used interchangeably.
Examples of foundational models generative AI systems include image generators (such as Midjourney or Stable Diffusion), large language models (such as OpenAI’s Chat GPT-4, Google’s PaLM, or Anthropic’s Claude), code generation tools (such as Copilot), or audio generation tools (such as VALL-E or resemble.ai).

## Identifications

This thematic area measures steps countries have taken to respond to the risks and opportunities posed by Generative AI technologies. In particular, evidence must account for (1) frameworks relating to Generative AI, (2) government actions with the object of disseminating or enforcing rights or obligations related to Generative AI, and (3) actors outside government working to advance, enable or enforce responsible use of Generative AI.
Frameworks may take the form of adopted policies, laws, regulations, or guidelines. Government actions may include draft laws or policies, the establishment of expert working groups to provide policy recommendations or guidance, the creation of
oversight bodies to spread awareness or gather more data around it. Non-state actors (NSAs) may include non-governmental organisations (NGOs), but also multinational corporations, private military organisations, media outlets, organised ethnic groups, academic institutions, lobby groups, labour unions or social movementsworking to advance, enable or enforce responsible use of Generative AI.


## Some Examples:

### Frameworks:

In July 2023, China issued a set of temporary measures that requires service providers of generative AI products to conduct security assessments and to perform algorithm filing procedures. This followed three months after China’s Office of the Central Cyberspace Affairs Commission released a draft of its Measures for Generative Artificial Intelligence Services (Measures) in April 2023 which aimed to govern products that use generative AI models, like ChatGPT. The draft Measures set out the rules that service providers must follow, which includes restrictions of the type of content these products are allowed to generate and measures that must be taken to protect against misinformation and abuse, algorithmic bias and discrimination, and to enhance transparency. In addition, the draft Measures obligate “Providers” of generative AI technologies to submit security assessment reports to relevant authorities to demonstrate the legitimacy of the date used to train generative AI products and compliance with procedures for algorithm filing, modification and cancellation prior to offering services to the general public.

### Government Actions

The US Senate Judiciary Subcommittee on Privacy, Technology and the Law held a series of hearings on the oversight of AI in May 2023 in an effort to hold new technologies accountable. The first hearing was centered around generative AI and the sensitivity surrounding its development. Testimonies by Open AI’s CEO, Sam Altman, IBM Vice President Christina Montgomery and New York University Professor Gary Marcus, provided a chance to explain generative AI and gave legislators an opportunity to express their reservations about its impact on society, the economy, and elections.

### Non-state Actors

The Centre of AI and Digital Policy (CAIDP), an organization based in Washington DC, filed a complaint against OpenAI stating that GPT-4 violates rules against unfair and deceptive practices, and that OpenAI’s release of AI-text generation tools like GPT-4 is biased, deceptive and poses risks to public safety. The organization highlights malicious code, tailored propaganda, and biased training data as potential threats of GPT-4. In its complaint, the CAIDP calls on the Federal Trade Commission (FTC) to halt any further commercial deployment of GPT models and require assessment of these models before any further rollouts. They also ask for a publicly accessible reporting tool, transparency and enhanced privacy measures surrounding powerful tools like the GPT-4.
