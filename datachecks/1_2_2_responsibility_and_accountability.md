
# Data Check - Thematic area: Responsibility and Accountability

## Definitions

Responsibility can be defined as ‘the state or fact of being answerable, or accountable
for something within one's power, control, or management’. In the context of AI, the
concept of responsibility concerns the humans or decision-making body (whether
public or private) to whom causal relations to a certain outcome may be assigned. This
is different from the concept of ‘responsible AI’ in that it identifies who (i.e. humans,
legal entities, etc.) is responsible for what (i.e., decisions, outcomes, impacts), rather
than ensuring the use of AI in a safe, ethical and trustworthy way.
A closely related concept is accountability, which concerns both the provision of
information about, and the justification or explanation of, one’s conduct regarding the
design, development, deployment or use of an AI system to an authority. While more
precise technical definitions exist, various frameworks currently define accountability
as ‘the idea that one is responsible for their actions… and must be able to explain their
aims, motivations, and reasons’.
Further, according to the Organisation for Economic Cooperation and Development
(OECD), accountability generally ‘implies an ethical, moral, or other expectation… that
guides the actions or conduct of individuals or organisations and compels them to
provide the reasons for which decisions and actions were taken’. The notion of liability
addresses the legal dimension of accountability and refers to the legal consequences
of an action or inaction that has caused, or contributed to, a harm or negative
outcome.

## Identifications

This thematic area measures steps countries have taken to facilitate the responsibility
and accountability of AI systems, and to ensure a certain degree of control in the
design and operation of AI systems. In particular, evidence must account for (1)
frameworks concerning provisions or mechanisms for responsibility and
accountability, (2) government actions regulating or promoting implementation of
responsibility and accountability in AI systems through legal or other means, and (3)
non-state actors involved in promoting the principles of responsibility and
accountability and establishing regulatory provisions to facilitate the implementation
thereof.
Frameworks may take the form of laws, regulations, policies (including by sector
and/or department) and/or guidelines. Government actions may include the
development of draft laws or policies, the establishment of government bodies or
working groups to provide policy recommendations for creating mechanisms for
responsibility and accountability of AI systems as well as the implementation of laws
and/or policies that seek to promote adherence to this foundational principle. Nonstate actors (NSAs) may include non-governmental organisations (NGOs), but also
multinational corporations, private military organisations, media outlets, organised
ethnic groups, academic institutions, lobby groups, labour unions or social movements
working to advance or strengthen principles for responsibility and accountability in
the use of AI technologies, particularly amongst marginalised groups or communities.

## Some Examples:

### Frameworks:

In September 2022, the European Commission published its proposal for a Directive of
the European Parliament and Council on adapting non-contractual civil liability rules
to AI, known as the Artificial Intelligence Liability Directive (AILD). The AILD seeks to
enhance the accountability of AI systems by establishing a set of rules to govern
certain aspects of non-contractual civil liabilities for damages caused by AI systems.
The AILD will do this by: (1) removing the barriers to legitimate liability claims by
establishing disclosure requirements on high-risk technologies to the provider of AI
systems (Article 3); and (2) reducing the burden of proof linked with damages caused
by AI systems by introducing a presumption of causal link (Article 4).

### Government Actions

The European Parliamentary Research Service (EPRS) provides research support to
Members of the European Parliament to ensure Members are capacitated with a
comprehensive analysis of, and research on, issues relating to the European Union, to
assist them in their parliamentary work. Since the publication of the AILD, EPRS has
released updated summary briefings throughout the legislative process to capture
discussions on ‘the adequacy and effectiveness of the proposed liability regime, its
coherence with the artificial intelligence act currently under negotiation, its potential
detrimental impact on innovation, and the interplay between EU and national rules’.

### Non-state Actors

The Future of Life Institute (FLI), an independent non-profit organisation based in
Belgium, works to advance policy development for human-centred technologies by
‘steer[ing] transformative technology towards benefitting life and away from extreme
large-scale risks’. In response to the AILD, FLI published a position paper that called
for the introduction of a strict liability regime for general purpose and high-risk AI
systems and suggested a fault-based liability regime for all other AI systems where
the presumption of fault lies on the operator. In addition, the FLI also called on EU
lawmakers ‘to harmonise the immaterial damages and indirect harms (e.g. freedom of
expression, human dignity, discrimination) for which compensation is allowed’, ‘to
define the specific types of damages that would be recoverable in EU law’, and ‘to
include clear rules on allocating liability for damages caused by AI systems across the
value chain.’ The EPRS cited the FLI’s position paper in its latest briefing to
parliamentary Members.
